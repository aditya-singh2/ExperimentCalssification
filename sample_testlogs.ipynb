{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f75c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['EXPERIMENT_DETAILS']= '{\"name\": \"Airline_GradientBoost_10k\", \"algo_details\": {\"sklearn.ensemble.GradientBoostingClassifier\": null}, \"id\": \"394\", \"dataset\": \"AIRLINE_DEP_DELAY_10K\", \"target_column\": \"DEP_DEL15\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c9ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Experiment Execution with the following params:\n",
      "{\"name\": \"snowparkanalysis00\", \"algo_details\": {\"sklearn.ensemble.ExtraTreesClassifier\": null}, \"id\": \"367\", \"dataset\": \"EMPLOYEE_10L\", \"target_column\": \"LEAVEORNOT\"}\n",
      "\n",
      "INFO:root:Inside experiment\n",
      "INFO:root:Reading datset from snowflake\n",
      "Reading dataframe from snowflake native connector\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.6.0, Python Version: 3.8.18, Platform: Linux-6.1.58+-x86_64-with-glibc2.17\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n",
      "INFO:snowflake.connector.cursor:query: [use warehouse FOSFOR_INSIGHT_WH;]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:Number of results in first chunk: 1\n",
      "INFO:snowflake.connector.cursor:query: [SELECT * FROM \"EMPLOYEE_10L\"]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:Number of results in first chunk: 0\n",
      "INFO:snowflake.connector.connection:closed\n",
      "INFO:snowflake.connector.connection:No async queries seem to be running, deleting session\n",
      "INFO:root:Starting Preprocessing\n",
      "['EDUCATION', 'CITY', 'GENDER', 'EVERBENCHED'] columns are non numeric in feature dataset, encoding required.\n",
      "Columns identified to be encoded with label encoder: []\n",
      "Columns identified to be encoded with one hot encoder: ['EDUCATION', 'CITY', 'GENDER', 'EVERBENCHED']\n",
      "new one hot encoded df: [[1. 0. 0. ... 1. 1. 0.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 1. 1. 0.]\n",
      " [1. 0. 0. ... 1. 0. 1.]\n",
      " [1. 0. 0. ... 1. 0. 1.]]\n",
      "one hot encoder object: OneHotEncoder()\n",
      "\n",
      "final feature df created:          JOININGYEAR  PAYMENTTIER  AGE  EXPERIENCEINCURRENTDOMAIN  \\\n",
      "0               2017            3   34                          0   \n",
      "1               2013            1   28                          3   \n",
      "2               2014            3   38                          2   \n",
      "3               2016            3   27                          5   \n",
      "4               2017            3   24                          2   \n",
      "...              ...          ...  ...                        ...   \n",
      "1191163         2013            3   26                          4   \n",
      "1191164         2013            2   37                          2   \n",
      "1191165         2018            3   27                          5   \n",
      "1191166         2012            3   30                          2   \n",
      "1191167         2015            3   33                          4   \n",
      "\n",
      "         EDUCATION_Bachelors  EDUCATION_Masters  EDUCATION_PHD  \\\n",
      "0                        1.0                0.0            0.0   \n",
      "1                        1.0                0.0            0.0   \n",
      "2                        1.0                0.0            0.0   \n",
      "3                        0.0                1.0            0.0   \n",
      "4                        0.0                1.0            0.0   \n",
      "...                      ...                ...            ...   \n",
      "1191163                  1.0                0.0            0.0   \n",
      "1191164                  0.0                1.0            0.0   \n",
      "1191165                  0.0                1.0            0.0   \n",
      "1191166                  1.0                0.0            0.0   \n",
      "1191167                  1.0                0.0            0.0   \n",
      "\n",
      "         CITY_Bangalore  CITY_New Delhi  CITY_Pune  GENDER_Female  \\\n",
      "0                   1.0             0.0        0.0            0.0   \n",
      "1                   0.0             0.0        1.0            1.0   \n",
      "2                   0.0             1.0        0.0            1.0   \n",
      "3                   1.0             0.0        0.0            0.0   \n",
      "4                   0.0             0.0        1.0            0.0   \n",
      "...                 ...             ...        ...            ...   \n",
      "1191163             1.0             0.0        0.0            1.0   \n",
      "1191164             0.0             0.0        1.0            0.0   \n",
      "1191165             0.0             1.0        0.0            0.0   \n",
      "1191166             1.0             0.0        0.0            0.0   \n",
      "1191167             1.0             0.0        0.0            0.0   \n",
      "\n",
      "         GENDER_Male  EVERBENCHED_No  EVERBENCHED_Yes  \n",
      "0                1.0             1.0              0.0  \n",
      "1                0.0             1.0              0.0  \n",
      "2                0.0             1.0              0.0  \n",
      "3                1.0             1.0              0.0  \n",
      "4                1.0             0.0              1.0  \n",
      "...              ...             ...              ...  \n",
      "1191163          0.0             1.0              0.0  \n",
      "1191164          1.0             1.0              0.0  \n",
      "1191165          1.0             1.0              0.0  \n",
      "1191166          1.0             0.0              1.0  \n",
      "1191167          1.0             0.0              1.0  \n",
      "\n",
      "[1191168 rows x 14 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/mlflow/data/dataset_source_registry.py:150: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n",
      "  return _dataset_source_registry.resolve(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting experiment run for sklearn.ensemble.ExtraTreesClassifier with hyperparam: {'n_estimators': [100], 'criterion': ['gini', 'entropy'], 'max_features': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
      "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ]), 'min_samples_split': range(2, 21), 'min_samples_leaf': range(1, 21), 'bootstrap': [True, False]}\n",
      "Starting experiment run for sklearn.ensemble.ExtraTreesClassifier with hyperparam: {'n_estimators': [100], 'criterion': ['gini', 'entropy'], 'max_features': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
      "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ]), 'min_samples_split': range(2, 21), 'min_samples_leaf': range(1, 21), 'bootstrap': [True, False]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 0.12.1 of tpot is outdated. Version 0.12.2 was released Friday February 23, 2024.\n",
      "/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607e1bae95c143619bed91ecf1903489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/120 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cloudpickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import (accuracy_score, average_precision_score, f1_score,\n",
    "                             precision_score, recall_score, roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow.sklearn\n",
    "from fosforio import get_local_dataframe, get_dataframe\n",
    "\n",
    "import logging, sys\n",
    "# Initilization\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "def experiment(exp_details, tpot_config, generations, population_size, cv, random_state, verbosity):\n",
    "    \"\"\"\n",
    "    exp_details: {\n",
    "        name: exp_name\n",
    "        id: exp_id\n",
    "        description: exp_description\n",
    "        dataset: exp_dataset\n",
    "        target_column: exp_target_column\n",
    "        algo_details: None\n",
    "    }\n",
    "    tpot_config: dict\n",
    "    generations: 0.5\n",
    "    population_size: 0.5\n",
    "    cv: 5\n",
    "    random_state: 42\n",
    "    verbosity: 2\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(\"Inside experiment\")\n",
    "    \n",
    "    # Tracking URI set\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URL\", \"http://mlflow-server\"))\n",
    "\n",
    "    # Setting experiment name\n",
    "    mlflow.set_experiment(exp_details.get(\"name\", \"sample_experiment\"))\n",
    "\n",
    "    # Adding description to the experiment\n",
    "    tags = {'mlflow.note.content': exp_details.get(\"description\", \"sample_description\")}\n",
    "\n",
    "    # Reading input data\n",
    "    try:\n",
    "        if exp_details.get(\"dataset\").split(\".\")[-1].lower() in [\"csv\", \"tsv\", \"xlsx\", \"xls\"]:\n",
    "            # Read the input data file from /data mount attached to this notebook pod\n",
    "            logger.info(\"Reading datset from /data\")\n",
    "            input_file = \"/data/\" + exp_details.get(\"dataset\")\n",
    "            data = get_local_dataframe(input_file)\n",
    "        else:\n",
    "            # Input data is a dataset\n",
    "            logger.info(\"Reading datset from snowflake\")\n",
    "            data = get_dataframe(exp_details.get(\"dataset\"))\n",
    "            # Adding this to log input data in mlflow\n",
    "            input_file = '/tmp/input_data.csv'\n",
    "            data.to_csv(input_file, index=False)\n",
    "    except Exception as ex:\n",
    "        logger.error(f\"Unable to read input dataset.\\nError: {ex}\")\n",
    "        print(f\"Unable to read input dataset.\\nError: {ex}\")\n",
    "\n",
    "    # Data Preprocessing: Validating and encoding the data if required and imputing null values.\n",
    "    logger.info(\"Starting Preprocessing\")\n",
    "    data = data.fillna(method='pad')  # Filling null values with the previous ones\n",
    "    data = data.fillna(method='bfill')  # Filling null value with the next ones\n",
    "    df_target, le_target, df_feature, le_dict_feature, oh_enc_feature, le_column_feature, oh_column_feature = encoding(\n",
    "        data, exp_details.get(\"target_column\"))\n",
    "\n",
    "    # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "    train_x, test_x, train_y, test_y = train_test_split(df_feature, df_target, train_size=0.75, test_size=0.25)\n",
    "\n",
    "    # Registering datasets with mlflow experiment run\n",
    "    dataset = mlflow.data.from_pandas(data, source=input_file)\n",
    "\n",
    "    for algo, hyperparam in exp_details.get(\"algo_details\").items():\n",
    "        if not hyperparam and type(hyperparam) != dict:\n",
    "            hyperparam = TPOTClassifier.default_config_dict[algo]\n",
    "        logger.info(f\"Starting experiment run for {algo} with hyperparam: {hyperparam}\")\n",
    "        print(f\"Starting experiment run for {algo} with hyperparam: {hyperparam}\")\n",
    "        # Adding individual algorithm and its hyperparam to tpot config along with preprocessors and selectors\n",
    "        tpot_config_dict = {**tpot_config, **{algo: hyperparam}}\n",
    "        with mlflow.start_run(tags=tags):\n",
    "            pipeline_optimizer = TPOTClassifier(\n",
    "                generations=generations,\n",
    "                population_size=population_size,\n",
    "                cv=cv,\n",
    "                random_state=random_state,\n",
    "                verbosity=verbosity,\n",
    "                config_dict=tpot_config_dict\n",
    "            )\n",
    "\n",
    "            pipeline_optimizer.fit(train_x, train_y)\n",
    "\n",
    "            predicted_qualities = pipeline_optimizer.predict(test_x)\n",
    "\n",
    "            matrices = eval_metrics(test_y, predicted_qualities)\n",
    "            \n",
    "            logger.info(f\"{str(pipeline_optimizer.fitted_pipeline_)} algorithms best selected by TPOT\")\n",
    "            print(f\"{str(pipeline_optimizer.fitted_pipeline_)} algorithms best selected by TPOT\")\n",
    "            print(f\"  score: {pipeline_optimizer.score(test_x, test_y)}\")\n",
    "\n",
    "            try:\n",
    "                # logging hyper params for the best run/pipeline chosen\n",
    "                for step, _ in pipeline_optimizer.fitted_pipeline_.named_steps.items():\n",
    "                    # Checking if the step used in TPOT pipeline is present in TPOT default config dict.\n",
    "                    # if yes, then log only the hyperparams which are present in TPOT default config and not all.\n",
    "                    step_name = [s for s in TPOTClassifier.default_config_dict.keys() if step.lower() in s.lower()]\n",
    "                    for k, v in pipeline_optimizer.fitted_pipeline_.named_steps[step].get_params().items():\n",
    "                        if step_name:\n",
    "                            if k in TPOTClassifier.default_config_dict.get(step_name[0]):\n",
    "                                mlflow.log_param(str(step) + \"_\" + str(k), v)\n",
    "                        else:\n",
    "                            mlflow.log_param(str(step) + \"_\" + str(k), v)\n",
    "            except Exception as ex:\n",
    "                print(f\"Exception occurred in logging params to mlflow.\\nex: {ex}\")\n",
    "\n",
    "            # logging model metric\n",
    "            for i in matrices:\n",
    "                if matrices[i]:\n",
    "                    mlflow.log_metric(i, matrices[i])\n",
    "                    print(i, matrices[i])\n",
    "            mlflow.log_metric(\"score\", pipeline_optimizer.score(test_x, test_y))\n",
    "\n",
    "            # Log input data to MLflow run artifact.\n",
    "            mlflow.log_artifact(input_file)\n",
    "\n",
    "            # Registering datasets with mlflow experiment run\n",
    "            mlflow.log_input(dataset, context=\"input\")\n",
    "\n",
    "            # Set custom tags\n",
    "            mlflow.set_tags({\n",
    "                \"template_id\": os.getenv(\"template_id\", \"sample_template_id\"),\n",
    "                \"notebook_name\": os.getenv(\"notebook_name\", \"sample_notebook_name\"),\n",
    "                \"algorithm\": algo,\n",
    "                \"algo_details\": exp_details.get(\"algo_details\"),\n",
    "                \"tpot_selected_algo\": str(pipeline_optimizer.fitted_pipeline_)\n",
    "            })\n",
    "\n",
    "            predictions = pipeline_optimizer.fitted_pipeline_.predict(train_x)\n",
    "            signature = infer_signature(train_x, predictions)\n",
    "\n",
    "            # Storing score function for the model\n",
    "            score_and_dump_func(\"/tmp/scoring_func\")\n",
    "            mlflow.log_artifact(\"/tmp/scoring_func\")\n",
    "\n",
    "            # Register the model\n",
    "            mlflow.sklearn.log_model(\n",
    "                pipeline_optimizer.fitted_pipeline_, \"model\",\n",
    "                registered_model_name=exp_details.get(\"name\", \"sample_experiment\"), signature=signature,\n",
    "                pip_requirements=['mlflow==2.10.0', 'sqlalchemy==1.3.5']\n",
    "            )\n",
    "\n",
    "            # Exporting the autogenerated code of tpot for best pipeline\n",
    "            pipeline_optimizer.export(f'tpot_exported_{exp_details.get(\"name\")}_{algo}.py')\n",
    "            mlflow.log_artifact(f'tpot_exported_{exp_details.get(\"name\")}_{algo}.py')\n",
    "\n",
    "\n",
    "def try_or(fn):\n",
    "    try:\n",
    "        out = fn()\n",
    "        return out\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def eval_metrics(y_actual, y_pred):\n",
    "    \"\"\"\n",
    "    :param\n",
    "    actual\n",
    "    pred\n",
    "    :returns\n",
    "    rmse, mae, r2\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"accuracy_score\": try_or(lambda: accuracy_score(y_actual, y_pred)),\n",
    "        \"average_precision_score\": try_or(lambda: average_precision_score(y_actual, y_pred)),\n",
    "        \"f1_score\": try_or(lambda: f1_score(y_actual, y_pred, average=\"weighted\", labels=np.unique(y_pred))),\n",
    "        \"precision_score\": try_or(\n",
    "            lambda: precision_score(y_actual, y_pred, average=\"weighted\", labels=np.unique(y_pred))),\n",
    "        \"recall_score\": try_or(lambda: recall_score(y_actual, y_pred, average=\"weighted\")),\n",
    "        \"roc_auc_score\": try_or(lambda: roc_auc_score(y_actual, y_pred))\n",
    "    }\n",
    "\n",
    "\n",
    "def score_and_dump_func(file_path):\n",
    "    \"\"\"\n",
    "    :param\n",
    "    file_path\n",
    "    \"\"\"\n",
    "\n",
    "    def score_func(model, request):\n",
    "        \"\"\"\n",
    "        :param\n",
    "        model\n",
    "        request\n",
    "        :returns\n",
    "        score_output\n",
    "        \"\"\"\n",
    "        # Enter your custom score function here\n",
    "\n",
    "        score_output = \"Success\"\n",
    "        return score_output\n",
    "\n",
    "    with open(file_path, \"wb\") as out:\n",
    "        cloudpickle.dump(score_func, out)\n",
    "\n",
    "\n",
    "def encoding(df, target_column):\n",
    "    \"\"\"\n",
    "    Checking whether encoding required in target and feature datasets.\n",
    "    If required, then encoding them with label and one hot encoding.\n",
    "    :param:\n",
    "    df: input dataframe\n",
    "    target_column: target column\n",
    "    :returns:\n",
    "    df_target: target dataframe\n",
    "    le_target: target label encoder object\n",
    "    df_feature: feature dataframe\n",
    "    le_dict_feature: dict of feature label encoder objects\n",
    "    oh_enc_feature: feature one hot encoder object\n",
    "    le_column_feature: list of feature label encoder columns\n",
    "    oh_column_feature: list of feature one hot encoder columns\n",
    "    \"\"\"\n",
    "    df_target = df[[target_column]]\n",
    "    le_target = None\n",
    "    # Target column validation and encoding\n",
    "    if df.dtypes[target_column].name in ['object', 'bool']:\n",
    "        print(f\"target_column is of {df.dtypes[target_column].name} datatype, encoding required.\")\n",
    "        le_target = LabelEncoder()\n",
    "        df_target[target_column] = pd.DataFrame(le_target.fit_transform(df_target[target_column].astype(str)))\n",
    "        print(f\"Target column label encoded {df_target[target_column]}, object: {le_target}\")\n",
    "\n",
    "    # Feature column validation and encoding\n",
    "    df_feature = df.drop(target_column, axis=1)\n",
    "    non_numeric_cols = df_feature.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "    le_dict_feature = {}\n",
    "    le_column_feature = []\n",
    "    oh_column_feature = []\n",
    "    oh_enc_feature = None\n",
    "    if len(non_numeric_cols) >= 1:\n",
    "        print(f\"{non_numeric_cols} columns are non numeric in feature dataset, encoding required.\")\n",
    "        for col in non_numeric_cols:\n",
    "            if df_feature[col].nunique() >= 10:\n",
    "                le_column_feature.append(col)\n",
    "            else:\n",
    "                oh_column_feature.append(col)\n",
    "\n",
    "        print(f\"Columns identified to be encoded with label encoder: {le_column_feature}\\n\"\n",
    "              f\"Columns identified to be encoded with one hot encoder: {oh_column_feature}\")\n",
    "\n",
    "        # columns to be label encoded\n",
    "        if len(le_column_feature) == 0:\n",
    "            df_feature = df_feature\n",
    "        else:\n",
    "            for col in le_column_feature:\n",
    "                le_dict_feature[col] = LabelEncoder()\n",
    "                df_feature[col] = le_dict_feature[col].fit_transform(df_feature[col].astype(str))\n",
    "                print(f\"{col} column label encoded {df_feature[col]}, object: {le_dict_feature[col]}\")\n",
    "\n",
    "        # columns to be one hot encoded\n",
    "        if len(oh_column_feature) == 0:\n",
    "            df_feature = df_feature\n",
    "        else:\n",
    "            unique_combinations = pd.get_dummies(df_feature[oh_column_feature])\n",
    "            unique_combinations_list = unique_combinations.columns.tolist()\n",
    "            oh_enc_feature = OneHotEncoder()\n",
    "            oh_encoded_array = oh_enc_feature.fit_transform(df_feature[oh_column_feature]).toarray() if len(\n",
    "                oh_column_feature) > 1 else oh_enc_feature.fit_transform(df_feature[oh_column_feature]).toarray()\n",
    "            df_oh_enc = pd.DataFrame(oh_encoded_array, columns=unique_combinations_list)\n",
    "            df_feature = df_feature.drop(columns=oh_column_feature)\n",
    "            df_feature = df_feature.join(df_oh_enc)\n",
    "            print(f\"new one hot encoded df: {oh_encoded_array}\\n\"\n",
    "                  f\"one hot encoder object: {oh_enc_feature}\\n\")\n",
    "        print(f\"final feature df created: {df_feature}\")\n",
    "    return df_target, le_target, df_feature, le_dict_feature, oh_enc_feature, le_column_feature, oh_column_feature\n",
    "\n",
    "\n",
    "# Adding Preprocessors and Selectors with tpot default hyperparameter tuning\n",
    "preprocessors_selectors = [\n",
    "    # Preprocessors\n",
    "    \"sklearn.preprocessing.Binarizer\",\n",
    "    \"sklearn.decomposition.FastICA\",\n",
    "    \"sklearn.cluster.FeatureAgglomeration\",\n",
    "    \"sklearn.preprocessing.MaxAbsScaler\",\n",
    "    \"sklearn.preprocessing.MinMaxScaler\",\n",
    "    \"sklearn.preprocessing.Normalizer\",\n",
    "    \"sklearn.kernel_approximation.Nystroem\",\n",
    "    \"sklearn.decomposition.PCA\",\n",
    "    \"sklearn.preprocessing.PolynomialFeatures\",\n",
    "    \"sklearn.kernel_approximation.RBFSampler\",\n",
    "    \"sklearn.preprocessing.RobustScaler\",\n",
    "    \"sklearn.preprocessing.StandardScaler\",\n",
    "    \"tpot.builtins.ZeroCount\",\n",
    "    \"tpot.builtins.OneHotEncoder\",\n",
    "    # Selectors\n",
    "    \"sklearn.feature_selection.SelectFwe\",\n",
    "    \"sklearn.feature_selection.SelectPercentile\",\n",
    "    \"sklearn.feature_selection.VarianceThreshold\",\n",
    "    \"sklearn.feature_selection.RFE\",\n",
    "    \"sklearn.feature_selection.SelectFromModel\"\n",
    "]\n",
    "tpot_config = {key: TPOTClassifier.default_config_dict[key] for key in preprocessors_selectors}\n",
    "\n",
    "# Running Experiment with user configured params.\n",
    "print(f\"Starting Experiment Execution with the following params:\\n{os.getenv('EXPERIMENT_DETAILS')}\\n\")\n",
    "experiment(exp_details=json.loads(os.getenv(\"EXPERIMENT_DETAILS\")), tpot_config=tpot_config, generations=5,\n",
    "           population_size=20, cv=5, random_state=42, verbosity=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37145f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
