{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f75c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 µs, sys: 6 µs, total: 32 µs\n",
      "Wall time: 34.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "os.environ['EXPERIMENT_DETAILS']= '{\"name\": \"Airline_GradientBoost_10k\", \"algo_details\": {\"sklearn.ensemble.GradientBoostingClassifier\": null}, \"id\": \"394\", \"dataset\": \"AIRLINE_DEP_DELAY_10K\", \"target_column\": \"DEP_DEL15\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c9ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection manager service url initialised to http://fdc-project-manager:80/project-manager\n",
      "If you need to update its value then update the variable CONNECTION_MANAGER_BASE_URL in os env.\n",
      "Starting Experiment Execution with the following params:\n",
      "{\"name\": \"Airline_GradientBoost_10k\", \"algo_details\": {\"sklearn.ensemble.GradientBoostingClassifier\": null}, \"id\": \"394\", \"dataset\": \"AIRLINE_DEP_DELAY_10K\", \"target_column\": \"DEP_DEL15\"}\n",
      "\n",
      "INFO:root:Inside experiment\n",
      "INFO:root:Reading datset from snowflake\n",
      "Reading dataframe from snowflake native connector\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.6.0, Python Version: 3.8.18, Platform: Linux-6.1.58+-x86_64-with-glibc2.17\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n",
      "INFO:snowflake.connector.cursor:query: [use warehouse FOSFOR_INSIGHT_WH;]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:Number of results in first chunk: 1\n",
      "INFO:snowflake.connector.cursor:query: [SELECT * FROM \"AIRLINE_DEP_DELAY_10K\"]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:Number of results in first chunk: 485\n",
      "INFO:snowflake.connector.connection:closed\n",
      "INFO:snowflake.connector.connection:No async queries seem to be running, deleting session\n",
      "INFO:root:Starting Preprocessing\n",
      "['DEP_TIME_BLK', 'CARRIER_NAME', 'DEPARTING_AIRPORT', 'PREVIOUS_AIRPORT'] columns are non numeric in feature dataset, encoding required.\n",
      "Columns identified to be encoded with label encoder: ['DEP_TIME_BLK', 'CARRIER_NAME', 'DEPARTING_AIRPORT', 'PREVIOUS_AIRPORT']\n",
      "Columns identified to be encoded with one hot encoder: []\n",
      "DEP_TIME_BLK column label encoded 0        5\n",
      "1        5\n",
      "2        5\n",
      "3        4\n",
      "4        4\n",
      "        ..\n",
      "9995    13\n",
      "9996    17\n",
      "9997     8\n",
      "9998     2\n",
      "9999    15\n",
      "Name: DEP_TIME_BLK, Length: 10000, dtype: int64, object: LabelEncoder()\n",
      "CARRIER_NAME column label encoded 0        6\n",
      "1        7\n",
      "2       16\n",
      "3        0\n",
      "4       14\n",
      "        ..\n",
      "9995     6\n",
      "9996     6\n",
      "9997     0\n",
      "9998    13\n",
      "9999    14\n",
      "Name: CARRIER_NAME, Length: 10000, dtype: int64, object: LabelEncoder()\n",
      "DEPARTING_AIRPORT column label encoded 0       10\n",
      "1       37\n",
      "2       85\n",
      "3       42\n",
      "4       44\n",
      "        ..\n",
      "9995    14\n",
      "9996    81\n",
      "9997    81\n",
      "9998    17\n",
      "9999    22\n",
      "Name: DEPARTING_AIRPORT, Length: 10000, dtype: int64, object: LabelEncoder()\n",
      "PREVIOUS_AIRPORT column label encoded 0        14\n",
      "1        17\n",
      "2       155\n",
      "3       216\n",
      "4       259\n",
      "       ... \n",
      "9995     14\n",
      "9996    244\n",
      "9997    210\n",
      "9998    234\n",
      "9999    223\n",
      "Name: PREVIOUS_AIRPORT, Length: 10000, dtype: int64, object: LabelEncoder()\n",
      "final feature df created:       MONTH  DAY_OF_WEEK  DEP_TIME_BLK  DISTANCE_GROUP  SEGMENT_NUMBER  \\\n",
      "0         4            6             5               3               3   \n",
      "1         3            2             5               2               2   \n",
      "2         2            5             5               8               2   \n",
      "3         4            6             4              11               2   \n",
      "4         8            7             4               1               2   \n",
      "...     ...          ...           ...             ...             ...   \n",
      "9995      5            5            13               3               3   \n",
      "9996      8            3            17               1               5   \n",
      "9997      2            5             8               5               3   \n",
      "9998      1            3             2               2               2   \n",
      "9999     10            1            15               2               6   \n",
      "\n",
      "      CONCURRENT_FLIGHTS  NUMBER_OF_SEATS  CARRIER_NAME  \\\n",
      "0                      5              110             6   \n",
      "1                     31               50             7   \n",
      "2                     37              173            16   \n",
      "3                     37              181             0   \n",
      "4                     27              143            14   \n",
      "...                  ...              ...           ...   \n",
      "9995                  71              157             6   \n",
      "9996                  12              110             6   \n",
      "9997                  19              181             0   \n",
      "9998                  24               76            13   \n",
      "9999                  17              143            14   \n",
      "\n",
      "      AIRPORT_FLIGHTS_MONTH  AIRLINE_FLIGHTS_MONTH  ...  PREVIOUS_AIRPORT  \\\n",
      "0                      6837                  81803  ...                14   \n",
      "1                     14450                  22191  ...                17   \n",
      "2                     16530                  43512  ...               155   \n",
      "3                     17522                  21136  ...               216   \n",
      "4                     14220                 114987  ...               259   \n",
      "...                     ...                    ...  ...               ...   \n",
      "9995                  25360                  85579  ...                14   \n",
      "9996                  14049                  91062  ...               244   \n",
      "9997                   9017                  18154  ...               210   \n",
      "9998                  11784                  62105  ...               234   \n",
      "9999                   8879                 115051  ...               223   \n",
      "\n",
      "      PRCP  SNOW  SNWD   TMAX   AWND  CARRIER_HISTORICAL  DEP_AIRPORT_HIST  \\\n",
      "0      0.0   0.0   0.0   64.0  13.87            0.139558          0.180307   \n",
      "1      0.0   0.0   0.0   48.0  12.08            0.155571          0.187883   \n",
      "2      0.0   0.0   0.0   54.0  10.07            0.189496          0.240316   \n",
      "3      0.0   0.0   0.0   64.0   7.38            0.132326          0.112840   \n",
      "4      0.0   0.0   0.0  104.0   4.92            0.192550          0.123487   \n",
      "...    ...   ...   ...    ...    ...                 ...               ...   \n",
      "9995   0.0   0.0   0.0   89.0  16.78            0.141341          0.172771   \n",
      "9996   0.0   0.0   0.0   89.0   7.16            0.168586          0.238865   \n",
      "9997   0.0   0.0   0.0   47.0   6.93            0.233099          0.230231   \n",
      "9998   0.0   0.0   5.1    1.0  19.46            0.208051          0.187883   \n",
      "9999   0.0   0.0   0.0   68.0   4.92            0.179323          0.161464   \n",
      "\n",
      "      DAY_HISTORICAL  DEP_BLOCK_HIST  \n",
      "0           0.177124        0.147773  \n",
      "1           0.132868        0.156045  \n",
      "2           0.236965        0.187484  \n",
      "3           0.177124        0.135374  \n",
      "4           0.196519        0.138067  \n",
      "...              ...             ...  \n",
      "9995        0.206977        0.309452  \n",
      "9996        0.221672        0.283996  \n",
      "9997        0.236965        0.246487  \n",
      "9998        0.180607        0.088475  \n",
      "9999        0.163571        0.217959  \n",
      "\n",
      "[10000 rows x 29 columns]\n",
      "INFO:root:Starting experiment run for sklearn.ensemble.GradientBoostingClassifier with hyperparam: {'n_estimators': [100], 'learning_rate': [0.001, 0.01, 0.1, 0.5, 1.0], 'max_depth': range(1, 11), 'min_samples_split': range(2, 21), 'min_samples_leaf': range(1, 21), 'subsample': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
      "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ]), 'max_features': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
      "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])}\n",
      "Starting experiment run for sklearn.ensemble.GradientBoostingClassifier with hyperparam: {'n_estimators': [100], 'learning_rate': [0.001, 0.01, 0.1, 0.5, 1.0], 'max_depth': range(1, 11), 'min_samples_split': range(2, 21), 'min_samples_leaf': range(1, 21), 'subsample': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
      "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ]), 'max_features': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
      "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/mlflow/data/dataset_source_registry.py:150: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n",
      "  return _dataset_source_registry.resolve(\n",
      "Version 0.12.1 of tpot is outdated. Version 0.12.2 was released Friday February 23, 2024.\n",
      "/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e18297c8354e8f89ee0ea5e95ce32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/120 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import json\n",
    "import cloudpickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import (accuracy_score, average_precision_score, f1_score,\n",
    "                             precision_score, recall_score, roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow.sklearn\n",
    "from fosforio import get_local_dataframe, get_dataframe\n",
    "\n",
    "import logging, sys\n",
    "# Initilization\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "def experiment(exp_details, tpot_config, generations, population_size, cv, random_state, verbosity):\n",
    "    \"\"\"\n",
    "    exp_details: {\n",
    "        name: exp_name\n",
    "        id: exp_id\n",
    "        description: exp_description\n",
    "        dataset: exp_dataset\n",
    "        target_column: exp_target_column\n",
    "        algo_details: None\n",
    "    }\n",
    "    tpot_config: dict\n",
    "    generations: 0.5\n",
    "    population_size: 0.5\n",
    "    cv: 5\n",
    "    random_state: 42\n",
    "    verbosity: 2\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(\"Inside experiment\")\n",
    "    \n",
    "    # Tracking URI set\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URL\", \"http://mlflow-server\"))\n",
    "\n",
    "    # Setting experiment name\n",
    "    mlflow.set_experiment(exp_details.get(\"name\", \"sample_experiment\"))\n",
    "\n",
    "    # Adding description to the experiment\n",
    "    tags = {'mlflow.note.content': exp_details.get(\"description\", \"sample_description\")}\n",
    "\n",
    "    # Reading input data\n",
    "    try:\n",
    "        if exp_details.get(\"dataset\").split(\".\")[-1].lower() in [\"csv\", \"tsv\", \"xlsx\", \"xls\"]:\n",
    "            # Read the input data file from /data mount attached to this notebook pod\n",
    "            logger.info(\"Reading datset from /data\")\n",
    "            input_file = \"/data/\" + exp_details.get(\"dataset\")\n",
    "            data = get_local_dataframe(input_file)\n",
    "        else:\n",
    "            # Input data is a dataset\n",
    "            logger.info(\"Reading datset from snowflake\")\n",
    "            data = get_dataframe(exp_details.get(\"dataset\"))\n",
    "            # Adding this to log input data in mlflow\n",
    "            input_file = '/tmp/input_data.csv'\n",
    "            data.to_csv(input_file, index=False)\n",
    "    except Exception as ex:\n",
    "        logger.error(f\"Unable to read input dataset.\\nError: {ex}\")\n",
    "        print(f\"Unable to read input dataset.\\nError: {ex}\")\n",
    "\n",
    "    # Data Preprocessing: Validating and encoding the data if required and imputing null values.\n",
    "    logger.info(\"Starting Preprocessing\")\n",
    "    data = data.fillna(method='pad')  # Filling null values with the previous ones\n",
    "    data = data.fillna(method='bfill')  # Filling null value with the next ones\n",
    "    df_target, le_target, df_feature, le_dict_feature, oh_enc_feature, le_column_feature, oh_column_feature = encoding(\n",
    "        data, exp_details.get(\"target_column\"))\n",
    "\n",
    "    # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "    train_x, test_x, train_y, test_y = train_test_split(df_feature, df_target, train_size=0.75, test_size=0.25)\n",
    "\n",
    "    # Registering datasets with mlflow experiment run\n",
    "    dataset = mlflow.data.from_pandas(data, source=input_file)\n",
    "\n",
    "    for algo, hyperparam in exp_details.get(\"algo_details\").items():\n",
    "        if not hyperparam and type(hyperparam) != dict:\n",
    "            hyperparam = TPOTClassifier.default_config_dict[algo]\n",
    "        logger.info(f\"Starting experiment run for {algo} with hyperparam: {hyperparam}\")\n",
    "        print(f\"Starting experiment run for {algo} with hyperparam: {hyperparam}\")\n",
    "        # Adding individual algorithm and its hyperparam to tpot config along with preprocessors and selectors\n",
    "        tpot_config_dict = {**tpot_config, **{algo: hyperparam}}\n",
    "        with mlflow.start_run(tags=tags):\n",
    "            pipeline_optimizer = TPOTClassifier(\n",
    "                generations=generations,\n",
    "                population_size=population_size,\n",
    "                cv=cv,\n",
    "                random_state=random_state,\n",
    "                verbosity=verbosity,\n",
    "                config_dict=tpot_config_dict\n",
    "            )\n",
    "\n",
    "            pipeline_optimizer.fit(train_x, train_y)\n",
    "\n",
    "            predicted_qualities = pipeline_optimizer.predict(test_x)\n",
    "\n",
    "            matrices = eval_metrics(test_y, predicted_qualities)\n",
    "            \n",
    "            logger.info(f\"{str(pipeline_optimizer.fitted_pipeline_)} algorithms best selected by TPOT\")\n",
    "            print(f\"{str(pipeline_optimizer.fitted_pipeline_)} algorithms best selected by TPOT\")\n",
    "            print(f\"  score: {pipeline_optimizer.score(test_x, test_y)}\")\n",
    "\n",
    "            try:\n",
    "                # logging hyper params for the best run/pipeline chosen\n",
    "                for step, _ in pipeline_optimizer.fitted_pipeline_.named_steps.items():\n",
    "                    # Checking if the step used in TPOT pipeline is present in TPOT default config dict.\n",
    "                    # if yes, then log only the hyperparams which are present in TPOT default config and not all.\n",
    "                    step_name = [s for s in TPOTClassifier.default_config_dict.keys() if step.lower() in s.lower()]\n",
    "                    for k, v in pipeline_optimizer.fitted_pipeline_.named_steps[step].get_params().items():\n",
    "                        if step_name:\n",
    "                            if k in TPOTClassifier.default_config_dict.get(step_name[0]):\n",
    "                                mlflow.log_param(str(step) + \"_\" + str(k), v)\n",
    "                        else:\n",
    "                            mlflow.log_param(str(step) + \"_\" + str(k), v)\n",
    "            except Exception as ex:\n",
    "                print(f\"Exception occurred in logging params to mlflow.\\nex: {ex}\")\n",
    "\n",
    "            # logging model metric\n",
    "            for i in matrices:\n",
    "                if matrices[i]:\n",
    "                    mlflow.log_metric(i, matrices[i])\n",
    "                    print(i, matrices[i])\n",
    "            mlflow.log_metric(\"score\", pipeline_optimizer.score(test_x, test_y))\n",
    "\n",
    "            # Log input data to MLflow run artifact.\n",
    "            mlflow.log_artifact(input_file)\n",
    "\n",
    "            # Registering datasets with mlflow experiment run\n",
    "            mlflow.log_input(dataset, context=\"input\")\n",
    "\n",
    "            # Set custom tags\n",
    "            mlflow.set_tags({\n",
    "                \"template_id\": os.getenv(\"template_id\", \"sample_template_id\"),\n",
    "                \"notebook_name\": os.getenv(\"notebook_name\", \"sample_notebook_name\"),\n",
    "                \"algorithm\": algo,\n",
    "                \"algo_details\": exp_details.get(\"algo_details\"),\n",
    "                \"tpot_selected_algo\": str(pipeline_optimizer.fitted_pipeline_)\n",
    "            })\n",
    "\n",
    "            predictions = pipeline_optimizer.fitted_pipeline_.predict(train_x)\n",
    "            signature = infer_signature(train_x, predictions)\n",
    "\n",
    "            # Storing score function for the model\n",
    "            score_and_dump_func(\"/tmp/scoring_func\")\n",
    "            mlflow.log_artifact(\"/tmp/scoring_func\")\n",
    "\n",
    "            # Register the model\n",
    "            mlflow.sklearn.log_model(\n",
    "                pipeline_optimizer.fitted_pipeline_, \"model\",\n",
    "                registered_model_name=exp_details.get(\"name\", \"sample_experiment\"), signature=signature,\n",
    "                pip_requirements=['mlflow==2.10.0', 'sqlalchemy==1.3.5']\n",
    "            )\n",
    "\n",
    "            # Exporting the autogenerated code of tpot for best pipeline\n",
    "            pipeline_optimizer.export(f'tpot_exported_{exp_details.get(\"name\")}_{algo}.py')\n",
    "            mlflow.log_artifact(f'tpot_exported_{exp_details.get(\"name\")}_{algo}.py')\n",
    "\n",
    "\n",
    "def try_or(fn):\n",
    "    try:\n",
    "        out = fn()\n",
    "        return out\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def eval_metrics(y_actual, y_pred):\n",
    "    \"\"\"\n",
    "    :param\n",
    "    actual\n",
    "    pred\n",
    "    :returns\n",
    "    rmse, mae, r2\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"accuracy_score\": try_or(lambda: accuracy_score(y_actual, y_pred)),\n",
    "        \"average_precision_score\": try_or(lambda: average_precision_score(y_actual, y_pred)),\n",
    "        \"f1_score\": try_or(lambda: f1_score(y_actual, y_pred, average=\"weighted\", labels=np.unique(y_pred))),\n",
    "        \"precision_score\": try_or(\n",
    "            lambda: precision_score(y_actual, y_pred, average=\"weighted\", labels=np.unique(y_pred))),\n",
    "        \"recall_score\": try_or(lambda: recall_score(y_actual, y_pred, average=\"weighted\")),\n",
    "        \"roc_auc_score\": try_or(lambda: roc_auc_score(y_actual, y_pred))\n",
    "    }\n",
    "\n",
    "\n",
    "def score_and_dump_func(file_path):\n",
    "    \"\"\"\n",
    "    :param\n",
    "    file_path\n",
    "    \"\"\"\n",
    "\n",
    "    def score_func(model, request):\n",
    "        \"\"\"\n",
    "        :param\n",
    "        model\n",
    "        request\n",
    "        :returns\n",
    "        score_output\n",
    "        \"\"\"\n",
    "        # Enter your custom score function here\n",
    "\n",
    "        score_output = \"Success\"\n",
    "        return score_output\n",
    "\n",
    "    with open(file_path, \"wb\") as out:\n",
    "        cloudpickle.dump(score_func, out)\n",
    "\n",
    "\n",
    "def encoding(df, target_column):\n",
    "    \"\"\"\n",
    "    Checking whether encoding required in target and feature datasets.\n",
    "    If required, then encoding them with label and one hot encoding.\n",
    "    :param:\n",
    "    df: input dataframe\n",
    "    target_column: target column\n",
    "    :returns:\n",
    "    df_target: target dataframe\n",
    "    le_target: target label encoder object\n",
    "    df_feature: feature dataframe\n",
    "    le_dict_feature: dict of feature label encoder objects\n",
    "    oh_enc_feature: feature one hot encoder object\n",
    "    le_column_feature: list of feature label encoder columns\n",
    "    oh_column_feature: list of feature one hot encoder columns\n",
    "    \"\"\"\n",
    "    df_target = df[[target_column]]\n",
    "    le_target = None\n",
    "    # Target column validation and encoding\n",
    "    if df.dtypes[target_column].name in ['object', 'bool']:\n",
    "        print(f\"target_column is of {df.dtypes[target_column].name} datatype, encoding required.\")\n",
    "        le_target = LabelEncoder()\n",
    "        df_target[target_column] = pd.DataFrame(le_target.fit_transform(df_target[target_column].astype(str)))\n",
    "        print(f\"Target column label encoded {df_target[target_column]}, object: {le_target}\")\n",
    "\n",
    "    # Feature column validation and encoding\n",
    "    df_feature = df.drop(target_column, axis=1)\n",
    "    non_numeric_cols = df_feature.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "    le_dict_feature = {}\n",
    "    le_column_feature = []\n",
    "    oh_column_feature = []\n",
    "    oh_enc_feature = None\n",
    "    if len(non_numeric_cols) >= 1:\n",
    "        print(f\"{non_numeric_cols} columns are non numeric in feature dataset, encoding required.\")\n",
    "        for col in non_numeric_cols:\n",
    "            if df_feature[col].nunique() >= 10:\n",
    "                le_column_feature.append(col)\n",
    "            else:\n",
    "                oh_column_feature.append(col)\n",
    "\n",
    "        print(f\"Columns identified to be encoded with label encoder: {le_column_feature}\\n\"\n",
    "              f\"Columns identified to be encoded with one hot encoder: {oh_column_feature}\")\n",
    "\n",
    "        # columns to be label encoded\n",
    "        if len(le_column_feature) == 0:\n",
    "            df_feature = df_feature\n",
    "        else:\n",
    "            for col in le_column_feature:\n",
    "                le_dict_feature[col] = LabelEncoder()\n",
    "                df_feature[col] = le_dict_feature[col].fit_transform(df_feature[col].astype(str))\n",
    "                print(f\"{col} column label encoded {df_feature[col]}, object: {le_dict_feature[col]}\")\n",
    "\n",
    "        # columns to be one hot encoded\n",
    "        if len(oh_column_feature) == 0:\n",
    "            df_feature = df_feature\n",
    "        else:\n",
    "            unique_combinations = pd.get_dummies(df_feature[oh_column_feature])\n",
    "            unique_combinations_list = unique_combinations.columns.tolist()\n",
    "            oh_enc_feature = OneHotEncoder()\n",
    "            oh_encoded_array = oh_enc_feature.fit_transform(df_feature[oh_column_feature]).toarray() if len(\n",
    "                oh_column_feature) > 1 else oh_enc_feature.fit_transform(df_feature[oh_column_feature]).toarray()\n",
    "            df_oh_enc = pd.DataFrame(oh_encoded_array, columns=unique_combinations_list)\n",
    "            df_feature = df_feature.drop(columns=oh_column_feature)\n",
    "            df_feature = df_feature.join(df_oh_enc)\n",
    "            print(f\"new one hot encoded df: {oh_encoded_array}\\n\"\n",
    "                  f\"one hot encoder object: {oh_enc_feature}\\n\")\n",
    "        print(f\"final feature df created: {df_feature}\")\n",
    "    return df_target, le_target, df_feature, le_dict_feature, oh_enc_feature, le_column_feature, oh_column_feature\n",
    "\n",
    "\n",
    "# Adding Preprocessors and Selectors with tpot default hyperparameter tuning\n",
    "preprocessors_selectors = [\n",
    "    # Preprocessors\n",
    "    \"sklearn.preprocessing.Binarizer\",\n",
    "    \"sklearn.decomposition.FastICA\",\n",
    "    \"sklearn.cluster.FeatureAgglomeration\",\n",
    "    \"sklearn.preprocessing.MaxAbsScaler\",\n",
    "    \"sklearn.preprocessing.MinMaxScaler\",\n",
    "    \"sklearn.preprocessing.Normalizer\",\n",
    "    \"sklearn.kernel_approximation.Nystroem\",\n",
    "    \"sklearn.decomposition.PCA\",\n",
    "    \"sklearn.preprocessing.PolynomialFeatures\",\n",
    "    \"sklearn.kernel_approximation.RBFSampler\",\n",
    "    \"sklearn.preprocessing.RobustScaler\",\n",
    "    \"sklearn.preprocessing.StandardScaler\",\n",
    "    \"tpot.builtins.ZeroCount\",\n",
    "    \"tpot.builtins.OneHotEncoder\",\n",
    "    # Selectors\n",
    "    \"sklearn.feature_selection.SelectFwe\",\n",
    "    \"sklearn.feature_selection.SelectPercentile\",\n",
    "    \"sklearn.feature_selection.VarianceThreshold\",\n",
    "    \"sklearn.feature_selection.RFE\",\n",
    "    \"sklearn.feature_selection.SelectFromModel\"\n",
    "]\n",
    "tpot_config = {key: TPOTClassifier.default_config_dict[key] for key in preprocessors_selectors}\n",
    "\n",
    "# Running Experiment with user configured params.\n",
    "print(f\"Starting Experiment Execution with the following params:\\n{os.getenv('EXPERIMENT_DETAILS')}\\n\")\n",
    "experiment(exp_details=json.loads(os.getenv(\"EXPERIMENT_DETAILS\")), tpot_config=tpot_config, generations=5,\n",
    "           population_size=20, cv=5, random_state=42, verbosity=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37145f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
